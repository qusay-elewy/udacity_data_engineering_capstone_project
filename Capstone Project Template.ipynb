{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# US IMMIGRATION DATA MODEL\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This is the Uacity Data Engineering Capstone Project, which presents a usecase where both big data engineering concepts and technologies are utilized to bring useful insights out of large and diverse data sources. In this project, a data model is built to enable analysts and business users to answer different types of questions related to the immigration trends to the US.\n",
    "\n",
    "Questions like *is there a peak season for immigration?* *Do immigrants from warm countries prefer certain States versus those who are coming from cold countries?* *What is the percentage of immigrants with business visas versus those with tourist visas?* All these questions and many more can be easily answered once our data model is built and filled with data.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import psycopg2\n",
    "import datetime as dt\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from IPython.display import display\n",
    "from pyspark.sql.functions import udf, month, year, dayofweek, dayofmonth, weekofyear, \\\n",
    "                                    when, col, split, trim, upper, row_number, \\\n",
    "                                    monotonically_increasing_id             \n",
    "from datetime import timedelta, datetime\n",
    "from pyspark.sql.types import StringType, IntegerType, DoubleType, LongType, FloatType, DecimalType, DateType\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = \"/opt/conda/bin:/opt/spark-2.4.3-bin-hadoop2.7/bin:/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/jvm/java-8-openjdk-amd64/bin\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"/opt/spark-2.4.3-bin-hadoop2.7\"\n",
    "\n",
    "#Increasing the memory usage on the drive to to 15GB to avoid running out of memory\n",
    "spark = SparkSession.builder.config(\"spark.driver.memory\", \"15g\")\\\n",
    "                            .config(\"spark.sql.broadcastTimeout\", \"36000\")\\\n",
    "                            .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "pd.set_option('max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "This projects aims at building a star schema data model that allows for better undersanding of the immigration trends to the US.\n",
    "\n",
    "pyspark is used to process our data, and for this project, Spark locally installed on the Udacity workspace is utilized. However, utilizing Amazon EMR would offer much beter performance.\n",
    "\n",
    "Generated data model is saved in .parquet format.\n",
    "\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Our data comes from different sources and in different formats described as follows:\n",
    "\n",
    "* **I94 Immigration Data:** This data comes from the US National Tourism and Trade Office. The data comes in .sas format, and it has information about entries made to the US in 2016. The data also comes with labels descriptions file which provides additional information about the main dataset. More about this dataset can be found [here](trade.gov/national-travel-and-tourism-office).\n",
    "* **World Temperature Data:** This dataset came from Kaggle, and it keeps track of the global weather information. The data is provided as a .csv file. More about this dataset can be found [here](kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data).\n",
    "* **U.S. City Demographic Data:** This data is offered by OpenSoft, and it provides basic information about different city demographics. The data is also provided in .csv format. You can read more about it [here](public.opendatasoft.com/explore/dataset/us-cities-demographics/export/).\n",
    "* **Airport Code Table:** This is a simple table of airport codes and corresponding cities. The data is also provided in .csv format. It comes from [here](datahub.io/core/airport-codes#data).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess Data\n",
    "\n",
    "#### Explore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "|5748518.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     NV|20591.0|  32.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1|\n",
      "|5748519.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20582.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1|\n",
      "|5748520.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     F|  null|     DL|9.495645143E10|00040|      B1|\n",
      "|5748521.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  28.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1988.0|10292016|     M|  null|     DL|9.495638813E10|00040|      B1|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Reading the parquet data files from the sas_data folder using Spark which is installed on the workspace.\n",
    "i94_df = spark.read.load('./sas_data')\n",
    "i94_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperatures_df = spark.read.option(\"header\", \"true\").csv('../../data2/GlobalLandTemperaturesByCity.csv')\n",
    "temperatures_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|   California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|   New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demos_df = spark.read.option(\"header\", \"true\").option(\"delimiter\", \";\").csv(\"us-cities-demographics.csv\")\n",
    "demos_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_codes_df = spark.read.option(\"header\", \"true\").csv(\"airport-codes_csv.csv\")\n",
    "airport_codes_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Assess Data\n",
    "\n",
    "Gather information about our datasets and identify issues such as nulls and uneuseful fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+-----+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+-------+--------+\n",
      "|summary|  cicid|  i94yr| i94mon| i94cit| i94res|i94port|arrdate|i94mode|i94addr|depdate| i94bir|i94visa|  count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto| gender|insnum|airline| admnum|  fltno|visatype|\n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+-----+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+-------+--------+\n",
      "|  count|3096313|3096313|3096313|3096313|3096313|3096313|3096313|3096074|2943721|2953856|3095511|3096313|3096313| 3096312| 1215063| 8126|3096075|2957884|    392|2957884|3095511|3095836|2682044|113708|3012686|3096313|3076764| 3096313|\n",
      "+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+--------+-----+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94_df.summary(\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+-----------------------------+-------+-------+--------+---------+\n",
      "|summary|     dt|AverageTemperature|AverageTemperatureUncertainty|   City|Country|Latitude|Longitude|\n",
      "+-------+-------+------------------+-----------------------------+-------+-------+--------+---------+\n",
      "|  count|8599212|           8235082|                      8235082|8599212|8599212| 8599212|  8599212|\n",
      "+-------+-------+------------------+-----------------------------+-------+-------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperatures_df.summary(\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "|summary|City|State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|Race|Count|\n",
      "+-------+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "|  count|2891| 2891|      2891|           2888|             2888|            2891|              2878|        2878|                  2875|      2891|2891| 2891|\n",
      "+-------+----+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demos_df.summary(\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+-----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "|summary|ident| type| name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|coordinates|\n",
      "+-------+-----+-----+-----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "|  count|55075|55075|55075|       48069|    55075|      55075|     55075|       49399|   41030|     9189|     28686|      55075|\n",
      "+-------+-----+-----+-----+------------+---------+-----------+----------+------------+--------+---------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_codes_df.summary(\"count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 3: Define the Data Model\n",
    "\n",
    "Our data model represents a star schema that is composed of one fact table and six dimenssion tables described as follows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "| Table Name        | Columns                                        | Description                      | Type                         |\n",
    "| ----------------- | ---------------------------------------------- | -------------------------------- | ---------------------------- |\n",
    "| fact_I94          | cicid, i94yr, i94mon, i94cit, i94res, i94port, arrdate, i94mode, i94addr, depdate, i94bir, i94visa, dtadfile, gender, airline, fltno, visatype, stay                                       | Stores immigration records       | Fact table                            |\n",
    "| dim_I94visa       | id, type                                       | Stores visa types                | Dimenssion table             |\n",
    "| dim_I94mode       | id, mode                                       | Stores travel modes              | Dimenssion table             |\n",
    "| dim_I94port       | code, port, state_code                         | Stores ports                     | Dimenssion table             |\n",
    "| dim_Country       | code, country, average_temperature, latitude, longitude | Stores country information as well as temperatures     | Dimenssion table    |\n",
    "| dim_Demographics  | id, state, state_code, city, foreign_born, male_population, average_household_size, total_population, female_population, maiden_age, number_of_veterans, american_indian_and_alaska_native, asian, black_or_african_American, hispanic_or_latino, white | Stores city demographics | Dimenssion table |\n",
    "| dim_Date          | sasdate, isodate, year, month, week, day, dayofweek, isweekend, season | Stores data information | Dimenssion table |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "*The airport dataset was not used in this model as we could not find any useful usecases where such data can be used.*\n",
    "\n",
    "More details about these tables and the steps followed to process them are discussed in the following sections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "##### i94 Immigration Data\n",
    "\n",
    "From the statistics above, we can see that the values of the \"visapost\", \"occup\", \"entdepu\", \"insnum\" columns are mostly null, which means that they can be dropped without affecting our analyses.\n",
    "\n",
    "We also beleieve that columns \"count\", \"entdepd\", \"entdepa\", \"entdepu\", \"matflag\", \"dtaddto\", \"biryear\", \"admnum\" are not useful for our foreseable analyses, and therefore they will be dropped, as well.\n",
    "\n",
    "The rest of the columns are fine except for a few that contain some nulls so we will keep those columns but after dropping the null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_df = i94_df.drop(\"visapost\", \"occup\", \"entdepu\", \"insnum\")\\\n",
    "               .drop(\"count\", \"entdepd\", \"entdepa\", \"entdepu\", \"matflag\", \"dtaddto\", \"biryear\", \"admnum\")\n",
    "i94_df = i94_df.na.drop(subset=[\"airline\", \"gender\", \"i94addr\"])\n",
    "i94_df = i94_df.dropna(how=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "Calculating the length of stay can be useful for our analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Converts SAS dates to normal dates\n",
    "\"\"\"\n",
    "convert_date = udf(lambda x : x if x is None else (datetime(1960, 1, 1).date() + timedelta(x)).isoformat())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Calculate stay length (in days)\n",
    "stay = F.datediff(F.to_date(convert_date(i94_df.depdate)), F.to_date(convert_date(i94_df.arrdate)))\n",
    "i94_df = i94_df.withColumn(\"stay\", stay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "Fixing data types of our final schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "i94_df = i94_df.withColumn(\"cicid\", i94_df.cicid.cast(IntegerType()))\\\n",
    "                .withColumn(\"i94yr\", i94_df.i94yr.cast(IntegerType()))\\\n",
    "                .withColumn(\"i94mon\", i94_df.i94mon.cast(IntegerType()))\\\n",
    "                .withColumn(\"i94cit\", i94_df.i94cit.cast(IntegerType()))\\\n",
    "                .withColumn(\"i94res\", i94_df.i94res.cast(IntegerType()))\\\n",
    "                .withColumn(\"i94mode\", i94_df.i94mode.cast(IntegerType()))\\\n",
    "                .withColumn(\"i94bir\", i94_df.i94bir.cast(IntegerType()))\\\n",
    "                .withColumn(\"i94visa\", i94_df.i94visa.cast(IntegerType()))\\\n",
    "                .withColumn(\"arrdate\", i94_df.arrdate.cast(IntegerType()))\\\n",
    "                .withColumn(\"depdate\", i94_df.depdate.cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+--------+------+-------+-----+--------+----+\n",
      "|  cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|dtadfile|gender|airline|fltno|visatype|stay|\n",
      "+-------+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+--------+------+-------+-----+--------+----+\n",
      "|5748517| 2016|     4|   245|   438|    LOS|  20574|      1|     CA|  20582|    40|      1|20160430|     F|     QF|00011|      B1|   8|\n",
      "|5748518| 2016|     4|   245|   438|    LOS|  20574|      1|     NV|  20591|    32|      1|20160430|     F|     VA|00007|      B1|  17|\n",
      "|5748519| 2016|     4|   245|   438|    LOS|  20574|      1|     WA|  20582|    29|      1|20160430|     M|     DL|00040|      B1|   8|\n",
      "|5748520| 2016|     4|   245|   438|    LOS|  20574|      1|     WA|  20588|    29|      1|20160430|     F|     DL|00040|      B1|  14|\n",
      "|5748521| 2016|     4|   245|   438|    LOS|  20574|      1|     WA|  20588|    28|      1|20160430|     M|     DL|00040|      B1|  14|\n",
      "+-------+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+--------+------+-------+-----+--------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "##### fact_I94 Data Dictionary\n",
    "\n",
    "\n",
    "| Field Name  | Data Type   | Constraints                                      | Description                          |\n",
    "| ----------- | ------------| ------------------------------------------------ | ------------------------------------ |\n",
    "| cicid       | Integer     | PRIMARY KEY                                      | Unique identifier                    |\n",
    "| i94yr       | integer     | NOT NULL                                         | 4-digit year                         |\n",
    "| i94mon      | integer     | NOT NULL                                         | Month                                |\n",
    "| i94cit      | integer     | NOT NULL, FK (REFERENCES dim_County.code)        | Country of birth - 3-digit code      |\n",
    "| i94res      | integer     | NOT NULL, FK (REFERENCES dim_Country.code)       | Country of residency - 3-digit code  |\n",
    "| i94port     | integer     | NOT NULL, FK (REFERENCES dim_I94port.code)       | Entry port                           |\n",
    "| i94mode     | integer     | NOT NULL, FK (REFERENCES dim_I94mode.id)         | Transportation mode                  |\n",
    "| i94addr     | string      | NOT NULL, FK (REFERENCES dim_Demographics.state) | State of arrival                     |\n",
    "| i94visa     | integer     | NOT NULL, FK (REFERENCES dim_I94visa.id)         | Visa type                            |\n",
    "| i94bir      | integer     | NOT NULL                                         | Immigrant's age in years             |\n",
    "| gender      | string      | NOT NULL                                         | Immigrant's gender                   |\n",
    "| dtadfile    | integer     | NOT NULL                                         | Allowed until                        |\n",
    "| airline     | string      | NOT NULL                                         | Airline company                      |\n",
    "| fltno       | string      | NOT NULL                                         | Flight number                        |\n",
    "| visatype    | string      | NOT NULL                                         | Visa class                           |\n",
    "| arrdate     | integer     | NOT NULL, FK (REFERENCES dim_date.sasdate)       | Arrival date                         |\n",
    "| depdate     | integer     | NULL, FK (REFERENCES dim_date.sasdate)           | Departure date                       |\n",
    "| stay        | integer     | NULL                                             | Lenght of stay - in days             |\n",
    "|             |             |                                                  |                                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "##### i94 Immigration Labels Descriptions Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "As mentioned earlier, the immigration dataset comes with an additional dataset that provides information about the lookup values in the main dataset. We will use this dataset to build some lookup tables that are needed to complete our data model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Reading the labels .sas file\n",
    "with open('./I94_SAS_Labels_Descriptions.SAS') as f:\n",
    "    f_content = f.read()\n",
    "    f_content = f_content.replace('\\t', '')\n",
    "    \n",
    "def code_mapper(file, idx):\n",
    "    f_content2 = f_content[f_content.index(idx):]\n",
    "    f_content2 = f_content2[:f_content2.index(';')].split('\\n')\n",
    "    f_content2 = [i.replace(\"'\", \"\") for i in f_content2]\n",
    "    dic = [i.split('=') for i in f_content2[1:]]\n",
    "    dic = dict([i[0].strip(), i[1].strip()] for i in dic if len(i) == 2)\n",
    "    return dic\n",
    "\n",
    "#Extracting values\n",
    "i94cit_res = code_mapper(f_content, \"i94cntyl\")\n",
    "i94port = code_mapper(f_content, \"i94prtl\")\n",
    "i94mode = code_mapper(f_content, \"i94model\")\n",
    "i94addr = code_mapper(f_content, \"i94addrl\")\n",
    "i94visa = {'1':'Business', '2': 'Pleasure', '3' : 'Student'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "Now that we extracted our lists from the file, we can build our dataframes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Build a dataframe for the country codes\n",
    "\n",
    "i94cit_res_list = list(i94cit_res.items())\n",
    "i94cit_res_df = spark.createDataFrame(i94cit_res_list)\n",
    "\n",
    "#Fix column names and types\n",
    "i94cit_res_df = i94cit_res_df.withColumnRenamed(\"_1\", \"code\")\n",
    "i94cit_res_df = i94cit_res_df.withColumnRenamed(\"_2\", \"country\")\n",
    "i94cit_res_df = i94cit_res_df.withColumn(\"code\", i94cit_res_df.code.cast(IntegerType()))\n",
    "\n",
    "#Removing bad data\n",
    "i94cit_res_df = i94cit_res_df.dropna().drop_duplicates()\n",
    "i94cit_res_df = i94cit_res_df.filter((~F.lower(i94cit_res_df.country).contains('country')) & \\\n",
    "                     (~F.lower(i94cit_res_df.country).contains('invalid')) & \\\n",
    "                     (~F.lower(i94cit_res_df.country).contains('not show')))\\\n",
    "                        .orderBy(\"country\")\n",
    "\n",
    "#Fixing the name of Mexico so that it can be linked with the coresponding values in other tables\n",
    "i94cit_res_df = i94cit_res_df.withColumn(\"country\", \n",
    "                                         F.when(F.col(\"code\") == '582', \"MEXICO\").otherwise(F.col(\"country\")))\n",
    "\n",
    "#Fix data type\n",
    "i94cit_res_df = i94cit_res_df.withColumn(\"code\", i94cit_res_df.code.cast(IntegerType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|code|    country|\n",
      "+----+-----------+\n",
      "| 236|AFGHANISTAN|\n",
      "| 101|    ALBANIA|\n",
      "| 316|    ALGERIA|\n",
      "| 102|    ANDORRA|\n",
      "| 324|     ANGOLA|\n",
      "+----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94cit_res_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Build a dataframe for the ports\n",
    "\n",
    "i94port_list = list(i94port.items())\n",
    "i94port_df = spark.createDataFrame(i94port_list)\n",
    "\n",
    "i94port_df = i94port_df.withColumnRenamed(\"_1\", \"code\")\n",
    "i94port_df = i94port_df.withColumn(\"code\", trim(col(\"code\")))\n",
    "i94port_df = i94port_df.withColumn(\"port\", trim(split(col(\"_2\"), \", \").getItem(0)))\\\n",
    "            .withColumn(\"state_code\", trim(split(col(\"_2\"), \", \").getItem(1)))\\\n",
    "            .drop(\"_2\")\\\n",
    "            .dropDuplicates()\\\n",
    "            .dropna()\\\n",
    "            .orderBy(\"state_code\", \"port\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------+\n",
      "|code|                port|state_code|\n",
      "+----+--------------------+----------+\n",
      "| ALC|               ALCAN|        AK|\n",
      "| ANC|           ANCHORAGE|        AK|\n",
      "| BAR|BAKER AAF - BAKER...|        AK|\n",
      "| DAC|       DALTONS CACHE|        AK|\n",
      "| PIZ|DEW STATION PT LA...|        AK|\n",
      "+----+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94port_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "##### dim_I94port Data Dictionary\n",
    "\n",
    "| Field Name   | Data Type   | Constraints         | Description        |\n",
    "| ------------ | ----------- | ------------------- | ------------------ |\n",
    "| id           | integer     | PRIMARY KEY         | Unique identifier  |     \n",
    "| port         | string      | NOT NULL, UNIQUE    | Port name          |\n",
    "| state_code   | string      | NOT NULL            | State code         |\n",
    "|              |             |                     |                    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Build a dataframe for the modes\n",
    "\n",
    "i94mode_list = list(i94mode.items())\n",
    "i94mode_df = spark.createDataFrame(i94mode_list)\n",
    "\n",
    "#Fix column names and types\n",
    "i94mode_df = i94mode_df.withColumnRenamed(\"_1\", \"id\")\n",
    "i94mode_df = i94mode_df.withColumnRenamed(\"_2\", \"mode\")\n",
    "i94mode_df = i94mode_df.withColumn(\"id\", i94mode_df.id.cast(IntegerType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "| id|        mode|\n",
      "+---+------------+\n",
      "|  1|         Air|\n",
      "|  2|         Sea|\n",
      "|  3|        Land|\n",
      "|  9|Not reported|\n",
      "+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94mode_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "##### dim_I94mode Data Dictionary\n",
    "\n",
    "| Field Name   | Data Type    | Constraints      | Description         |\n",
    "| ------------ | -------------| ---------------- | ------------------- |\n",
    "| id           | integer      | PRIMARY KEY      | Unique identifier   |\n",
    "| mode         | string       | NOT NULL, UNIQUE | transportation mode |\n",
    "|              |              |                  |                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As we will discussed later, we will not use the i94addr dataset as is, but instead we are going to merge it with the demographics dataset to build our demographics table which will also contain the data extracted from i94addr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Build a datarame for the state codes\n",
    "\n",
    "i94addr_list = list(i94addr.items())\n",
    "i94addr_df = spark.createDataFrame(i94addr_list)\n",
    "\n",
    "#Fix column names and clean data\n",
    "i94addr_df = i94addr_df.withColumnRenamed(\"_1\", \"code\")\\\n",
    "                        .withColumnRenamed(\"_2\", \"state\")  \n",
    "i94addr_df = i94addr_df.dropna()\\\n",
    "                        .drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "|code|            state|\n",
      "+----+-----------------+\n",
      "|  WY|          WYOMING|\n",
      "|  TN|        TENNESSEE|\n",
      "|  KS|           KANSAS|\n",
      "|  IA|             IOWA|\n",
      "|  NV|           NEVADA|\n",
      "|  IN|          INDIANA|\n",
      "|  PR|      PUERTO RICO|\n",
      "|  NY|         NEW YORK|\n",
      "|  RI|     RHODE ISLAND|\n",
      "|  MN|        MINNESOTA|\n",
      "|  DC|DIST. OF COLUMBIA|\n",
      "|  SD|        S. DAKOTA|\n",
      "|  MI|         MICHIGAN|\n",
      "|  ME|            MAINE|\n",
      "|  AL|          ALABAMA|\n",
      "|  FL|          FLORIDA|\n",
      "|  PA|     PENNSYLVANIA|\n",
      "|  CT|      CONNECTICUT|\n",
      "|  ND|        N. DAKOTA|\n",
      "|  MA|    MASSACHUSETTS|\n",
      "+----+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94addr_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Build a dataframe for the visa types\n",
    "\n",
    "i94visa_list = list(i94visa.items())\n",
    "i94visa_df = spark.createDataFrame(i94visa_list)\n",
    "i94visa_df = i94visa_df.withColumnRenamed(\"_1\", \"id\")\\\n",
    "                        .withColumnRenamed(\"_2\", \"type\")\n",
    "i94visa_df = i94visa_df.withColumn(\"id\", i94visa_df.id.cast(IntegerType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|    type|\n",
      "+---+--------+\n",
      "|  1|Business|\n",
      "|  2|Pleasure|\n",
      "|  3| Student|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94visa_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "##### dim_I94visa Data Dictionary\n",
    "\n",
    "| Field Name | Data Type  | Constraints       | Description               |\n",
    "| -----------| -----------| ----------------- | ------------------------- |\n",
    "| id         | integer    | PRIMARY KEY       | Unique identifier         |\n",
    "| type       | string     | NOT NULL, UNIQUE  | Visa type                 |\n",
    "|            |            |                   |                           |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "##### Temperature Data\n",
    "\n",
    "The temperature table holds informaion up to 2013-09-01, which means that we can not get year-per-year information by linking it to the i94 immigration dataset. However, we can group the average temperatures by country and link it with the i94cit dataset and compose a more detailed country dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Drop nulls, duplicates and unuseful columns\n",
    "temperatures_df = temperatures_df.drop(\"dt\", \"AverageTemperatureUncertainty\", \"City\")\n",
    "temperatures_df = temperatures_df.dropna().drop_duplicates()\n",
    "\n",
    "#Capitalize the country field so that we could link it with i94cit dataset\n",
    "temperatures_df = temperatures_df.withColumn(\"Country\", upper(col(\"Country\")))\n",
    "\n",
    "#Group by country\n",
    "temperatures_df = temperatures_df.groupby(\"Country\")\\\n",
    "                                .agg({\"AverageTemperature\": \"mean\", \"Latitude\": \"first\", \"Longitude\": \"first\"})\\\n",
    "                                .orderBy(\"Country\")\n",
    "\n",
    "#Rename columns\n",
    "temperatures_df = temperatures_df.withColumnRenamed(\"Country\", \"country\")\\\n",
    "                                    .withColumnRenamed(\"avg(AverageTemperature)\", \"average_temperature\")\\\n",
    "                                    .withColumnRenamed(\"first(Latitude)\", \"latitude\")\\\n",
    "                                    .withColumnRenamed(\"first(Longitude)\", \"longitude\")\n",
    "\n",
    "#Fix data types\n",
    "temperatures_df = temperatures_df.withColumn(\"average_temperature\", temperatures_df.average_temperature.cast(DoubleType()))\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+--------+---------+\n",
      "|    country|average_temperature|latitude|longitude|\n",
      "+-----------+-------------------+--------+---------+\n",
      "|AFGHANISTAN|  14.15373113443278|  36.17N|   69.61E|\n",
      "|    ALBANIA| 15.525674512441158|  40.99N|   19.17E|\n",
      "|    ALGERIA| 17.787410937285568|  36.17N|    3.98E|\n",
      "|     ANGOLA| 21.446687647212784|  12.05S|   13.15E|\n",
      "|  ARGENTINA|     16.83114138155|  39.38S|   62.43W|\n",
      "+-----------+-------------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperatures_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "Now that temperatures_df is ready, we can build a unified, more detailed country dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Join i94cit_res_df and temperatures_df and drop the redundant colum\n",
    "countries_df = i94cit_res_df.join(temperatures_df,i94cit_res_df.country ==  temperatures_df.country,\"Left\")\\\n",
    "                            .drop(temperatures_df.country) #To avoid adding the common column twice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-------------------+--------+---------+\n",
      "|code|             country|average_temperature|latitude|longitude|\n",
      "+----+--------------------+-------------------+--------+---------+\n",
      "| 151|             ARMENIA|  8.363008838891123|  40.99N|   44.73E|\n",
      "| 512|             BAHAMAS|  24.70784696016771|  24.92N|   78.03W|\n",
      "| 373|        SOUTH AFRICA|  16.80486927079032|  26.52S|   28.66E|\n",
      "| 243|               BURMA| 25.652096833353696|  20.09N|   92.13E|\n",
      "| 726|HEARD AND MCDONAL...|               null|    null|     null|\n",
      "+----+--------------------+-------------------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### dim_Country Data Dictionary\n",
    "\n",
    "| Field Name           | Data Type   | Constraints      | Description        |\n",
    "| -------------------- | ----------- | ---------------- | -------------------- |\n",
    "| code                 | integer     | PRIMARY KEY      | Country code         |\n",
    "| country              | string      | NOT NULL, UNIQUE | Country name         |\n",
    "| average_temperature  | double      | NOT NULL         | Average temperature  |\n",
    "| latitude             | string      | NOT NULL         | latitude             |\n",
    "| longitude            | string      | NOT NULL         | longitude            |\n",
    "|                      |             |                  |                      | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Demographics Data\n",
    "\n",
    "This data provides useful information about city demographics. However, it is hard to use it in our model in the current form and thereforere, we are going to pivot the numeric valuues as a step in our transformation of this dataset.\n",
    "\n",
    "The data will be grouped by state_code, state, and city.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Rename the columns into a format that's easier to deal with while coding\n",
    "demos_df = demos_df.withColumnRenamed(\"Median Age\", \"median_age\")\\\n",
    "                    .withColumnRenamed(\"Male Population\", \"male_population\")\\\n",
    "                    .withColumnRenamed(\"Female Population\", \"female_population\")\\\n",
    "                    .withColumnRenamed(\"Total Population\", \"total_population\")\\\n",
    "                    .withColumnRenamed(\"City\", \"city\")\\\n",
    "                    .withColumnRenamed(\"State\", \"state\")\\\n",
    "                    .withColumnRenamed(\"State Code\", \"state_code\")\\\n",
    "                    .withColumnRenamed(\"Number of Veterans\", \"number_of_veterans\")\\\n",
    "                    .withColumnRenamed(\"Foreign-born\", \"foreign_born\")\\\n",
    "                    .withColumnRenamed(\"Average Household Size\", \"average_household_size\")\\\n",
    "                    .withColumnRenamed(\"Race\", \"race\")\\\n",
    "                    .withColumnRenamed(\"Count\", \"total\")\n",
    "\n",
    "#Changing the data types of the numeric columns from string to the corresponding data types. \n",
    "#This step is needed so that we could use aggregate functions with this data\n",
    "demos_df = demos_df.withColumn(\"median_age\",demos_df.median_age.cast(DoubleType()))\\\n",
    "                    .withColumn(\"average_household_size\",demos_df.average_household_size.cast(DoubleType()))\\\n",
    "                    .withColumn(\"male_population\",demos_df.male_population.cast(IntegerType()))\\\n",
    "                    .withColumn(\"female_population\",demos_df.female_population.cast(IntegerType()))\\\n",
    "                    .withColumn(\"total_population\",demos_df.total_population.cast(IntegerType()))\\\n",
    "                    .withColumn(\"number_of_veterans\",demos_df.number_of_veterans.cast(IntegerType()))\\\n",
    "                    .withColumn(\"foreign_born\",demos_df.foreign_born.cast(IntegerType()))\\\n",
    "                    .withColumn(\"count\",demos_df.total.cast(IntegerType()))\\\n",
    "                    .drop(\"total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Pivoting data\n",
    "\n",
    "#Group by state_code, state, and city\n",
    "fixed_df = demos_df.groupby([\"state\", \"city\"])\\\n",
    "                    .agg({\"state_code\": \"first\", \"median_age\": \"first\", \"male_population\": \"first\",\\\n",
    "                          \"female_population\": \"first\", \"total_population\": \"first\",\\\n",
    "                          \"number_of_veterans\": \"first\", \"foreign_born\": \"first\",\\\n",
    "                          \"average_household_size\": \"first\"})\n",
    "\n",
    "#Pivot our data by race\n",
    "pivot_df = demos_df.groupby([\"state\", \"city\"]).pivot(\"race\").sum(\"count\")\n",
    "\n",
    "#Join both dataframes and do the necessary transofrmation (i.e., rename columns, and fill null numeric values with 0)\n",
    "demos_df = fixed_df.join(other=pivot_df, on=[\"state\", \"city\"], how=\"inner\")\\\n",
    "                    .withColumnRenamed(\"American Indian and Alaska Native\", \"american_indian_and_alaska_native\")\\\n",
    "                    .withColumnRenamed(\"Asian\", \"asian\")\\\n",
    "                    .withColumnRenamed(\"Black or African-American\", \"black_or_african_American\")\\\n",
    "                    .withColumnRenamed(\"Hispanic or Latino\", \"hispanic_or_latino\")\\\n",
    "                    .withColumnRenamed(\"White\", \"white\")\\\n",
    "                    .withColumnRenamed(\"first(foreign_born)\", \"foreign_born\")\\\n",
    "                    .withColumnRenamed(\"first(male_population)\", \"male_population\")\\\n",
    "                    .withColumnRenamed(\"first(average_household_size)\", \"average_household_size\")\\\n",
    "                    .withColumnRenamed(\"first(total_population)\", \"total_population\")\\\n",
    "                    .withColumnRenamed(\"first(median_age)\", \"maiden_age\") \\\n",
    "                    .withColumnRenamed(\"first(number_of_veterans)\", \"number_of_veterans\")\\\n",
    "                    .withColumnRenamed(\"first(female_population)\", \"female_population\")\\\n",
    "                    .withColumnRenamed(\"first(state_code)\", \"state_code\")\\\n",
    "                    .na.fill(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------+---------------+----------------------+----------------+-----------------+----------+----------+------------------+---------------------------------+-----+-------------------------+------------------+------+\n",
      "|      state|       city|foreign_born|male_population|average_household_size|total_population|female_population|state_code|maiden_age|number_of_veterans|american_indian_and_alaska_native|asian|black_or_african_American|hispanic_or_latino| white|\n",
      "+-----------+-----------+------------+---------------+----------------------+----------------+-----------------+----------+----------+------------------+---------------------------------+-----+-------------------------+------------------+------+\n",
      "| California|     Orange|       34550|          67337|                  3.14|          140995|            73658|        CA|      35.0|              3993|                              658|22457|                     2853|             46255|100083|\n",
      "| California|Yorba Linda|       15532|          31960|                   3.0|           67966|            36006|        CA|      45.5|              3171|                              211|17616|                     1326|             10599| 49980|\n",
      "|Connecticut|  Waterbury|       19967|          52235|                  2.71|          108807|            56572|        CT|      36.2|              3493|                              492| 4080|                    26972|             38222| 69075|\n",
      "|  Louisiana|  Lafayette|        5396|          64018|                  2.54|          127661|            63643|        LA|      31.7|              5206|                             1183| 2776|                    39862|              6556| 85282|\n",
      "|   New York|    Yonkers|       61247|          96580|                   2.8|          201118|           104538|        NY|      38.0|              4801|                             1112|13981|                    38731|             73608|129492|\n",
      "+-----------+-----------+------------+---------------+----------------------+----------------+-----------------+----------+----------+------------------+---------------------------------+-----+-------------------------+------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demos_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "Both the i94addr and the demographics datasets store State information, which means we can join them together and form a single dataset for States. i94addr dataset has 55 states versus 49 states in the demographics table, which means that we should have a left join between the i94addr and demographics datasets, respectively, to make sure all States are covered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 States found in i94addr dataset vs. 49 States found in demographics dataset.\n"
     ]
    }
   ],
   "source": [
    "addr_count = i94addr_df.select(\"code\").distinct().count()\n",
    "demos_count = demos_df.select(\"state_code\").distinct().count()\n",
    "\n",
    "print(f\"{addr_count} States found in i94addr dataset vs. {demos_count} States found in demographics dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Redundat columns are deleted after the join is performed\n",
    "demos_df = i94addr_df.join(demos_df, i94addr_df.code == demos_df.state_code, \"Left\")\\\n",
    "                        .drop(i94addr_df.code)\\\n",
    "                        .drop(demos_df.state)\n",
    "\n",
    "#Create an id column to be used as a primary key\n",
    "#In a standard relational model, state_code and city could be used as composite primary key\n",
    "demos_df = demos_df.withColumn(\"id\", monotonically_increasing_id())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------------+---------------+----------------------+----------------+-----------------+----------+----------+------------------+---------------------------------+-----+-------------------------+------------------+-------+-----------+\n",
      "|  state|      city|foreign_born|male_population|average_household_size|total_population|female_population|state_code|maiden_age|number_of_veterans|american_indian_and_alaska_native|asian|black_or_african_American|hispanic_or_latino|  white|         id|\n",
      "+-------+----------+------------+---------------+----------------------+----------------+-----------------+----------+----------+------------------+---------------------------------+-----+-------------------------+------------------+-------+-----------+\n",
      "|ARIZONA|    Tucson|       82220|         264893|                  2.45|          531674|           266781|        AZ|      33.6|             38182|                            24409|24689|                    33900|            231025| 404342|34359738368|\n",
      "|ARIZONA|Scottsdale|       27207|         115712|                  2.17|          236844|           121132|        AZ|      46.9|             16798|                             7003|11555|                     5279|             24766| 205512|34359738369|\n",
      "|ARIZONA|   Phoenix|      300702|         786833|                  2.89|         1563001|           776168|        AZ|      33.8|             72388|                            41748|66403|                   132939|            669914|1161455|34359738370|\n",
      "|ARIZONA|  Glendale|       44133|         116795|                  2.89|          240114|           123319|        AZ|      34.4|             13241|                             4064|15245|                    16254|             83711| 202539|34359738371|\n",
      "|ARIZONA|      Yuma|       19326|          48298|                  2.64|           94145|            45847|        AZ|      33.4|              7182|                             1228| 1180|                     3731|             57054|  69691|34359738372|\n",
      "+-------+----------+------------+---------------+----------------------+----------------+-----------------+----------+----------+------------------+---------------------------------+-----+-------------------------+------------------+-------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demos_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "##### dim_Demographics Data Dictionary\n",
    "\n",
    "\n",
    "| Field Name                        | Data Type  | Constraints   | Description                          |\n",
    "| --------------------------------- | ---------- | ------------- | ------------------------------------ |\n",
    "| id                                | long       | PRIMARY KEY   | Unique identifier                    |\n",
    "| state_code                        | string     | NOT NULL      | State code                           |\n",
    "| state                             | string     | NOT NULL      | State name                           |\n",
    "| city                              | string     | NOT NULL      | City name                            |\n",
    "| average_household_size            | double     | NOT NULL      | Average household size               |\n",
    "| foreign_born                      | integer    | NOT NULL      | Number of foreign-born residents     |\n",
    "| maiden_age                        | double     | NOT NULL      | Maiden age                          |\n",
    "| number_of_veterans                | integer    | NOT NULL      | Number of vetran residents           |\n",
    "| male_population                   | integer    | NOT NULL      | Number of male populaion             |\n",
    "| female_population                 | integer    | NOT NULL      | Number of female population          |\n",
    "| total_population                  | integer    | NOT NULL      | Total number of population           |\n",
    "| american_indian_and_alaska_native | long       | NOT NULL      | Number of Native American residents  |\n",
    "| black_or_african_american         | long       | NOT NULL      | Number of African American residents |\n",
    "| hispanic_or_latino                | long       | NOT NULL      | Number of hispanic residents         |\n",
    "| white                             | long       | NOT NULL      | Number of white residents            |\n",
    "|                                   |            |               |                                      |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Although id field is used here as a primary key, what actually identify each raw is the combination of the State and City fields. That is, there is not two cities with the same name under one state - something like composite keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Date Data\n",
    "\n",
    "We need to have this table to complete our model. This dataframe will provide detailed information about both arrival and departure dates.\n",
    "First, we create a function to calculate the weather seasons of our dates, then we construct our dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "@udf(StringType())\n",
    "def get_season(x):\n",
    "    \"\"\"\n",
    "    Calculates the weather season of the passed month\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if (x == 12 or x == 1 or x == 2):\n",
    "            return \"Winter\"\n",
    "        elif (x == 3 or x == 4 or x == 5):\n",
    "            return \"Spring\"\n",
    "        elif (x == 6 or x == 7 or x == 8):\n",
    "            return \"Summer\"\n",
    "        else:\n",
    "            return \"Autumn\"\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Create a dataframe for dates\n",
    "\n",
    "#Get a unique list of both arrival and departure dates from i94_df, and then combine them together\n",
    "arrdates_df = i94_df.select(\"arrdate\").distinct()\n",
    "depdates_df = i94_df.select(\"depdate\").distinct()\n",
    "date_df = arrdates_df.union(depdates_df).dropDuplicates()\n",
    "date_df = date_df.withColumnRenamed(\"arrdate\", \"sasdate\")\n",
    "\n",
    "iso_date = convert_date(date_df.sasdate)\n",
    "\n",
    "dt = F.to_date(iso_date)\n",
    "year = F.year(dt)\n",
    "month = F.month(dt)\n",
    "day = F.dayofmonth(dt)\n",
    "week = F.weekofyear(dt)\n",
    "day_of_week = F.dayofweek(dt)\n",
    "is_weekend = day_of_week.isin([1,7]).cast(\"int\")\n",
    "\n",
    "#Compose the dataframe using the values extracted from the sasdate field\n",
    "date_df = date_df.withColumn(\"isodate\", dt.cast(DateType()))\\\n",
    "                    .withColumn(\"year\", year.cast(IntegerType()))\\\n",
    "                    .withColumn(\"month\", month.cast(IntegerType()))\\\n",
    "                    .withColumn(\"week\", week.cast(IntegerType()))\\\n",
    "                    .withColumn(\"day\", day.cast(IntegerType()))\\\n",
    "                    .withColumn(\"dayofweek\", day_of_week.cast(IntegerType()))\\\n",
    "                    .withColumn(\"isweekend\", is_weekend)\\\n",
    "                    .withColumn(\"season\", get_season(month))\\\n",
    "                    .dropna()\\\n",
    "                    .orderBy(\"isodate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----+-----+----+---+---------+---------+------+\n",
      "|sasdate|   isodate|year|month|week|day|dayofweek|isweekend|season|\n",
      "+-------+----------+----+-----+----+---+---------+---------+------+\n",
      "|  20226|2015-05-18|2015|    5|  21| 18|        2|        0|Spring|\n",
      "|  20383|2015-10-22|2015|   10|  43| 22|        5|        0|Autumn|\n",
      "|  20479|2016-01-26|2016|    1|   4| 26|        3|        0|Winter|\n",
      "|  20516|2016-03-03|2016|    3|   9|  3|        5|        0|Spring|\n",
      "|  20518|2016-03-05|2016|    3|   9|  5|        7|        1|Spring|\n",
      "+-------+----------+----+-----+----+---+---------+---------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "##### dim_Date Data Dictionary\n",
    "\n",
    "| Field Name           | Data Type  | Constraints      | Description                               |\n",
    "| -------------------- | -----------| ---------------- | ----------------------------------------- |\n",
    "| sasdate              | integer    | PRIMARY KEY      | Date in SAS format                        |\n",
    "| isodate              | date       | NOT NULL, UNIQUE | Date in ISO format                        |\n",
    "| year                 | integer    | NOT NULL         | Date year                                 |\n",
    "| month                | integer    | NOT NULL         | Date month                                |\n",
    "| week                 | integer    | NOT NULL         | Date week                                 |\n",
    "| day                  | integer    | NOT NULL         | Date day                                  |\n",
    "| dayofweek            | integer    | NOT NULL         | Number of the week in the year            |\n",
    "| isweekend            | integer    | NOT NULL         | Checks whether the date is weekend or not |\n",
    "| season               | string     | NOT NULL         | Weather season                            |\n",
    "|                      |            |                  |                                           |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Airports Data\n",
    "\n",
    "No common fields were found in this table that could be useful to our analysis, so it was not used in our data model. However, it was transformed and saved like the rest of the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#From the statistics shown earlier, we think the iata_code field is useless as it's mostly null\n",
    "airport_codes_df = airport_codes_df.drop(\"iata_code\")\n",
    "\n",
    "#Exctract States from iso_region\n",
    "airport_codes_df = airport_codes_df.withColumn(\"country\", split(col(\"iso_region\"), \"-\").getItem(0))\\\n",
    "                                    .withColumn(\"state\", split(col(\"iso_region\"), \"-\").getItem(1))\\\n",
    "                                    .drop(\"iso_country\")\\\n",
    "                                    .drop(\"iso_region\")\n",
    "\n",
    "#Split coordinates field into x and y fields\n",
    "airport_codes_df = airport_codes_df.withColumn(\"x_coordinate\", split(col(\"coordinates\"), \", \").getItem(0))\\\n",
    "                                    .withColumn(\"y_coordinate\", split(col(\"coordinates\"), \", \").getItem(1))\\\n",
    "                                    .drop(\"coordinates\")\n",
    "\n",
    "#Fix data types\n",
    "airport_codes_df = airport_codes_df.withColumn(\"elevation_ft\", airport_codes_df.elevation_ft.cast(IntegerType()))\\\n",
    "                                    .withColumn(\"x_coordinate\", airport_codes_df.x_coordinate.cast(DoubleType()))\\\n",
    "                                    .withColumn(\"y_coordinate\", airport_codes_df.y_coordinate.cast(DoubleType()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+------------+--------+----------+-------+-----+------------------+-----------------+\n",
      "|ident|         type|                name|elevation_ft|continent|municipality|gps_code|local_code|country|state|      x_coordinate|     y_coordinate|\n",
      "+-----+-------------+--------------------+------------+---------+------------+--------+----------+-------+-----+------------------+-----------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|    Bensalem|     00A|       00A|     US|   PA|-74.93360137939453|   40.07080078125|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|       Leoti|    00AA|      00AA|     US|   KS|       -101.473911|        38.704022|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|Anchor Point|    00AK|      00AK|     US|   AK|    -151.695999146|      59.94919968|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|     Harvest|    00AL|      00AL|     US|   AL|-86.77030181884766|34.86479949951172|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|     Newport|    null|      null|     US|   AR|        -91.254898|          35.6087|\n",
      "+-----+-------------+--------------------+------------+---------+------------+--------+----------+-------+-----+------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_codes_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: integer (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- x_coordinate: double (nullable = true)\n",
      " |-- y_coordinate: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_codes_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### dim_Airport Data Dictionary\n",
    "\n",
    "| Field Name     | Data Type  | Constraints    | Description                       |\n",
    "| ---------------|------------| -------------- | --------------------------------- |\n",
    "| ident          | integer    | PRIMARY KEY    | Row unique identifier             |\n",
    "| type           | date       | NOT NULL       | Type of airport                   |\n",
    "| name           | integer    | NOT NULL       | Name of airport                   |\n",
    "| elevation_ft   | integer    | NOT NULL       | Elevation - in feet               |\n",
    "| continent      | integer    | NOT NULL       | Continent where the airport is    |\n",
    "| municipality   | integer    | NOT NULL       | City where theh airport is        |\n",
    "| gps_code       | integer    | NOT NULL       | Airport's GPS code                |\n",
    "| local_code     | integer    | NOT NULL       | Airport's local code              |\n",
    "| country        | string     | NOT NULL       | Country where the airport is      |\n",
    "| x_coodinate    | double     | NOT NULL       | X coordinate                      |                                      \n",
    "| y_coordinate   | double     | NOT NULL       | Y coordinate                      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Data Quality Checks\n",
    "\n",
    "Here, we perform some data quality checks to see if our model is working and that our data is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validaing row count...\n",
      "fact_I94 table created successfuly. 2493086 records transferred.\n",
      "dim_Demographics table created successfully. 602 records transferred.\n",
      "dim_Country table migrated successfully. 236 records transferred successfully.\n",
      "dim_Airport table migrated successfully. 55075 records transferred successfully.\n",
      "dim_Visa table migrated successfully. 3 records transferred successfully.\n",
      "dim_Mode table migrated successfully. 4 records transferred.\n",
      "dim_Port table migrated successfully. 582 records transferred successfully.\n",
      "dim_Date table migrated successfully. 187 records transferred.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Validaing row count...\")\n",
    "\n",
    "i94_count = i94_df.count()\n",
    "if i94_count > 0:\n",
    "    print(f\"fact_I94 table created successfuly. {i94_count} records transferred.\")\n",
    "else:\n",
    "    print(\"No data found in fact_I94 table\")\n",
    "\n",
    "demos_count = demos_df.count()\n",
    "if demos_count > 0:\n",
    "    print(f\"dim_Demographics table created successfully. {demos_count} records transferred.\")\n",
    "else:\n",
    "    print(\"No data found in dim_Demographics table\")\n",
    "\n",
    "countries_count = countries_df.count()\n",
    "if countries_count > 0:\n",
    "    print(f\"dim_Country table migrated successfully. {countries_count} records transferred successfully.\")\n",
    "else:\n",
    "    print(\"No data found in dim_Country table\")\n",
    "\n",
    "airports_count = airport_codes_df.count()\n",
    "if airports_count  > 0:\n",
    "    print(f\"dim_Airport table migrated successfully. {airports_count} records transferred successfully.\")\n",
    "else:\n",
    "    print(\"No data found in dim_Airports table\")\n",
    "\n",
    "visas_count = i94visa_df.count()\n",
    "if visas_count > 0:\n",
    "    print(f\"dim_Visa table migrated successfully. {visas_count} records transferred successfully.\")\n",
    "else:\n",
    "    print(\"No data found in dim_Visa table\")\n",
    "\n",
    "modes_count = i94mode_df.count()\n",
    "if modes_count > 0:\n",
    "    print(f\"dim_Mode table migrated successfully. {modes_count} records transferred.\")\n",
    "else:\n",
    "    print(\"No data found in dim_Mode table\")\n",
    "    \n",
    "ports_count = i94port_df.count()\n",
    "if ports_count > 0:\n",
    "    print(f\"dim_Port table migrated successfully. {ports_count} records transferred successfully.\")\n",
    "else:\n",
    "    print(\"No data found in dim_Port table\")\n",
    "\n",
    "dates_count = date_df.count()\n",
    "if dates_count > 0:\n",
    "    print(f\"dim_Date table migrated successfully. {dates_count} records transferred.\")\n",
    "else:\n",
    "    print(\"No data found in dim_Date table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating unique keys...\n",
      "checking dim_Country. Pass? True\n",
      "checking dim_I94port. Pass? True\n",
      "checking dim_I94mode. Pass? True\n",
      "checking dim_I94visa. Pass? True\n",
      "checking dim_Date. Pass? True\n",
      "checking dim_Demographics. Pass? True\n"
     ]
    }
   ],
   "source": [
    "#Counts unique values in each datarame and compare them to the number of rows in that dataframe. Both numbers should match.\n",
    "\n",
    "print(\"Validating unique keys...\")\n",
    "\n",
    "country_dis_count = countries_df.select(\"code\").distinct().count()\n",
    "print(\"checking dim_Country. Pass?\", country_dis_count == countries_count)\n",
    "\n",
    "port_dis_count = i94port_df.select(\"code\").distinct().count()\n",
    "print(\"checking dim_I94port. Pass?\",  port_dis_count == ports_count)\n",
    "\n",
    "mode_dis_count = i94mode_df.select(\"id\").distinct().count()\n",
    "print(\"checking dim_I94mode. Pass?\", mode_dis_count == modes_count)\n",
    "\n",
    "visa_dis_count = i94visa_df.select(\"id\").distinct().count()\n",
    "print(\"checking dim_I94visa. Pass?\", visa_dis_count == visas_count)\n",
    "\n",
    "dates_dis_count = date_df.select(\"sasdate\").distinct().count()\n",
    "print(\"checking dim_Date. Pass?\", dates_dis_count == dates_count)\n",
    "\n",
    "demo_dis_count = demos_df.select(\"state\", \"city\").distinct().count()\n",
    "print(\"checking dim_Demographics. Pass?\", demo_dis_count == demos_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating data...\n",
      "Data validation passed. Changed made during data transformation have been persisted.\n"
     ]
    }
   ],
   "source": [
    "#Making sure that the changes made to i94cit_res_df during the cleansing process are persisted. Here's just to test one of those changes\n",
    "print(\"Validating data...\")\n",
    "\n",
    "invalid_df = countries_df[F.lower(countries_df[\"country\"]).contains(\"country\") |\\\n",
    "                           F.lower(countries_df[\"country\"]).contains(\"invalid\") |\\\n",
    "                           F.lower(countries_df[\"country\"]).contains(\"not show\")]\n",
    "if(invalid_df.count() == 0):\n",
    "    print(\"Data validation passed. Changed made during data transformation have been persisted.\")\n",
    "else:\n",
    "    print(\"Dta validation failed. Some of the changes made during data transformation have not been persisted.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "#### Save Data Model\n",
    "\n",
    "Extracted data is saved as .parquet files in the output drectory on this workspace as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data...\n",
      "fact_I94 table saved successfully.\n",
      "dim_Demographics table saved successfully.\n",
      "dim_Country table saved successfully.\n",
      "dim_Date table saved successfully.\n",
      "dim_I94addr table saved successfully.\n",
      "dim_I94mode table saved successfully.\n",
      "dim_I94visa table saved successfully.\n",
      "dim_Airport table saved successfully.\n",
      "8 tables saved.\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving data...\")\n",
    "save_count = 0\n",
    "try:\n",
    "    i94_df.write.mode(\"overwrite\").parquet(\"output/fact_i94.parquet\")\n",
    "    global save_count \n",
    "    save_count += 1\n",
    "    print(\"fact_I94 table saved successfully.\")\n",
    "except ex:\n",
    "    print(f\"Error occured while saving fact_I94 table. {ex}\")\n",
    "    \n",
    "try:\n",
    "    demos_df.write.partitionBy(\"state_code\", \"city\").mode(\"overwrite\").parquet(\"output/dim_demographics.parquet\")\n",
    "    global save_count \n",
    "    save_count += 1\n",
    "    print(\"dim_Demographics table saved successfully.\")\n",
    "except:\n",
    "    print(\"Error occured while saving dim_Demographics table.\")\n",
    "\n",
    "try:\n",
    "    countries_df.write.partitionBy(\"code\").mode(\"overwrite\").parquet(\"output/dim_country.parquet\")\n",
    "    global save_count\n",
    "    save_count += 1\n",
    "    print(\"dim_Country table saved successfully.\")\n",
    "except:\n",
    "    print(\"Error occured while saving dim_Country table.\")\n",
    "    \n",
    "try:\n",
    "    date_df.write.partitionBy(\"sasdate\").mode(\"overwrite\").parquet(\"output/dim_date.parquet\")\n",
    "    global save_count\n",
    "    save_count += 1\n",
    "    print(\"dim_Date table saved successfully.\")\n",
    "except:\n",
    "    print(\"Error occured while saving dim_Date table.\")\n",
    "    \n",
    "try:\n",
    "    i94addr_df.write.partitionBy(\"code\").mode(\"overwrite\").parquet(\"output/dim_i94addr.parquet\")\n",
    "    global save_count\n",
    "    save_count += 1\n",
    "    print(\"dim_I94addr table saved successfully.\")\n",
    "except:\n",
    "    print(\"Error occured while saving dim_I94addr table.\")\n",
    "    \n",
    "try:\n",
    "    i94mode_df.write.partitionBy(\"id\").mode(\"overwrite\").parquet(\"output/dim_i94mode.parquet\")\n",
    "    global save_count\n",
    "    save_count += 1\n",
    "    print(\"dim_I94mode table saved successfully.\")\n",
    "except:\n",
    "    print(\"Error while saving dim_I94mode table.\")\n",
    "    \n",
    "try:\n",
    "    i94visa_df.write.partitionBy(\"id\").mode(\"overwrite\").parquet(\"output/dim_i94visa.parquet\")\n",
    "    global save_count\n",
    "    save_count += 1\n",
    "    print(\"dim_I94visa table saved successfully.\")\n",
    "except:\n",
    "    print(\"Error while saving dim_I94visa table.\")\n",
    "    \n",
    "try:\n",
    "    airport_codes_df.write.partitionBy(\"ident\").mode(\"overwrite\").parquet(\"output/dim_airports.parquet\")\n",
    "    global save_count\n",
    "    save_count += 1\n",
    "    print(\"dim_Airport table saved successfully.\")\n",
    "except:\n",
    "    print(\"Error occured while saving dim_Airport table.\")\n",
    "\n",
    "print(f\"{save_count} tables saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run ETL to Model Data\n",
    "\n",
    "The ETL code is organized in a logical order where each code block represents a task or a number of related tasks. Necessary imports and configurations are done at the begining of the code. No further imports or installs are needed.\n",
    "\n",
    "To execute this ETL, you can run \"python3 etl.py\" in Terminal.\n",
    "\n",
    "Some data quality checks are performed after processing and saving data. Processed data will be saved in .parquet format in a separate folder in this project, namely *output*. Some partitioning is applied to the saved data to achieve better perfprmance during the data read operations.\n",
    "\n",
    "The ETL has been tested and completed successfully before submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 5: Complete Project Write Up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "This project presents a usecase where big data engineering concepts and technologies such as Apache Spark are used to solve big data problems.\n",
    "\n",
    "A star schema was chosen for this model due to its simplicity in comparison to normalized data. That is, star schema generally offers faster reads, which is very important for OLAP applications, as it requires less complex and less number of joins between entities to bring back the data together. In comparison to the snowflake schema, in which the dimenssion tables can be normalized by breaking them into smaller tables, star schema is also much simpler and overlly faster as it is less normalized.\n",
    "\n",
    "The frequency at which data should be refreshed depends on two factors: 1) The frequency at which the data changes. 2) The reporting cycle. We can start with a monthly update and go from there.\n",
    "\n",
    "The code written here is scalable regardless of the size of our data as long as the used cluster is powerful enough to handle the assigned workload. However, there are some important considerations in terms of scaling this project to accomodate more complex scenarios such as:\n",
    "\n",
    "* **If the data was increased by 100x.** \n",
    "In this project, the Spark instance installed locally on the workspace is used. However, local/stand-alone Spark may not offer the optimum performance for larger datasets. On the contrary, Amazon EMR can be used in such cases as it offers virtually unlimited amount of computing resources. Users can easily scale up or down the size of their cluster depending on the complexity and the amount of the data being processed.\n",
    "* **If the pipelines were run on a daily basis by 7am.**\n",
    "Apache Airflow would make an excellent choice for dealing with automated workflows. SLAs can be set in Apache Airflow to meet certain deadlines.\n",
    "* **If the database needed to be accessed by 100+ people.**\n",
    "Again, Amazon EMR would make a perfect solution for such scenarios due to the offered flexibility and electicity of cloud computing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Validating Data Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+-----+------+-----+--------+-----+------------+----------+------+--------+----+------+----+\n",
      "|  cicid|i94bir|i94yr|state|gender|fltno|dtadfile|fltno|birthcountry|   arrdate|season|    visa|mode|  port|stay|\n",
      "+-------+------+-----+-----+------+-----+--------+-----+------------+----------+------+--------+----+------+----+\n",
      "|  51159|    49| 2016|   NE|     M|CSDFD|20160401|CSDFD|        null|2016-04-01|Spring|Business| Air|BANGOR|   1|\n",
      "|  51160|    44| 2016|   NE|     M|CSDFD|20160401|CSDFD|        null|2016-04-01|Spring|Business| Air|BANGOR|   1|\n",
      "|4670699|    43| 2016|   TN|     M|08458|20160425|08458|     DENMARK|2016-04-25|Spring|Business| Air|BANGOR|   2|\n",
      "|4671459|    58| 2016|   TN|     F|08458|20160425|08458|     DENMARK|2016-04-25|Spring|Business| Air|BANGOR|   2|\n",
      "|4671460|    40| 2016|   TN|     M|8458C|20160425|8458C|     DENMARK|2016-04-25|Spring|Business| Air|BANGOR|   2|\n",
      "|4671461|    40| 2016|   TN|     M|8458C|20160425|8458C|     DENMARK|2016-04-25|Spring|Business| Air|BANGOR|   2|\n",
      "|2049259|    40| 2016|   GA|     M|CNTNM|20160411|CNTNM|     MOROCCO|2016-04-11|Spring|Business| Air|BANGOR|   5|\n",
      "|2049260|    45| 2016|   GA|     M|CNTNM|20160411|CNTNM|     MOROCCO|2016-04-11|Spring|Business| Air|BANGOR|   6|\n",
      "|2874787|    44| 2016|   KS|     M|5042C|20160416|5042C|     AUSTRIA|2016-04-16|Spring|Business| Air|BANGOR|   1|\n",
      "|1983363|    71| 2016|   GA|     M|CNTNM|20160411|CNTNM|      FRANCE|2016-04-11|Spring|Business| Air|BANGOR|   5|\n",
      "|1162042|    46| 2016|   NY|     M|GHMEI|20160407|GHMEI|      FRANCE|2016-04-07|Spring|Business| Air|BANGOR|  21|\n",
      "|1162043|    34| 2016|   NY|     M|GHMEI|20160407|GHMEI|      FRANCE|2016-04-07|Spring|Business| Air|BANGOR|  23|\n",
      "|5019031|    47| 2016|   FL|     M|VPCMG|20160427|VPCMG|      FRANCE|2016-04-27|Spring|Business| Air|BANGOR|   8|\n",
      "|5019032|    51| 2016|   FL|     M|VPCMG|20160427|VPCMG|      FRANCE|2016-04-27|Spring|Business| Air|BANGOR|  19|\n",
      "|3491877|    56| 2016|   AR|     M|FWWQC|20160419|FWWQC|      FRANCE|2016-04-19|Spring|Business| Air|BANGOR|   4|\n",
      "|3491878|    47| 2016|   AR|     M|FWWQC|20160419|FWWQC|      FRANCE|2016-04-19|Spring|Business| Air|BANGOR|   4|\n",
      "|3491879|    52| 2016|   AR|     M|FWWQC|20160419|FWWQC|      FRANCE|2016-04-19|Spring|Business| Air|BANGOR|   4|\n",
      "|3491880|    44| 2016|   AR|     M|FWWQC|20160419|FWWQC|      FRANCE|2016-04-19|Spring|Business| Air|BANGOR|   4|\n",
      "|3491881|    43| 2016|   AR|     M|FWWQC|20160419|FWWQC|      FRANCE|2016-04-19|Spring|Business| Air|BANGOR|   4|\n",
      "|3491882|    42| 2016|   AR|     M|FWWQC|20160419|FWWQC|      FRANCE|2016-04-19|Spring|Business| Air|BANGOR|   4|\n",
      "+-------+------+-----+-----+------+-----+--------+-----+------------+----------+------+--------+----+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Join out dataframes and replace foreign keys with actual value\n",
    "query_df = i94_df.join(date_df, i94_df.arrdate == date_df.sasdate)\\\n",
    "                    .join(countries_df, i94_df.i94cit == countries_df.code, \"left\")\\\n",
    "                    .join(i94visa_df, i94_df.i94visa == i94visa_df.id, \"left\")\\\n",
    "                    .join(i94mode_df, i94_df.i94mode == i94mode_df.id, \"left\")\\\n",
    "                    .join(i94port_df, i94_df.i94port == i94port_df.code, \"left\")\n",
    "\n",
    "query_df = query_df.select(i94_df.cicid,  i94_df.i94bir, i94_df. i94yr, i94_df.i94addr, \\\n",
    "                           i94_df.gender, i94_df.fltno, i94_df.dtadfile, i94_df.fltno, \\\n",
    "                           countries_df.country, date_df.isodate, date_df.season, \\\n",
    "                           i94visa_df.type, i94mode_df.mode, i94port_df.port, i94_df.stay)\\\n",
    "                    .withColumnRenamed(\"i94addr\", \"state\")\\\n",
    "                    .withColumnRenamed(\"country\", \"birthcountry\")\\\n",
    "                    .withColumnRenamed(\"isodate\", \"arrdate\")\\\n",
    "                    .withColumnRenamed(\"type\", \"visa\")\n",
    "                    \n",
    "query_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data completeness...\n",
      "Number of records returned from the test query: 2493086 records.\n",
      "Number of records found in i94_df: 2493086 records.\n",
      "Data complete? True\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking data completeness...\")\n",
    "query_count = query_df.count()\n",
    "print(f\"Number of records returned from the test query: {query_count} records.\")\n",
    "print(f\"Number of records found in i94_df: {i94_count} records.\")\n",
    "print(\"Data complete?\", query_count == i94_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "From these numbers, we can see that our star schema allowed us to bring back our data together with a simple query.\n",
    "\n",
    "Now, let's take one of the records returned from this query and analyze it.\n",
    "We can see that the record with cicid = 51160 has a null value in the birth_country field, so let's see if there was matching values in the original i94res list extracted from the labels descriptions sas file to validate that our table dim_Country has the correct information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+--------+------+-------+-----+--------+----+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|dtadfile|gender|airline|fltno|visatype|stay|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+--------+------+-------+-----+--------+----+\n",
      "|51160| 2016|     4|   148|   112|    BGM|  20545|      1|     NE|  20546|    44|      1|20160401|     M|    *GA|CSDFD|      B1|   1|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+--------+------+-------+-----+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94_df.select(\"*\").where(i94_df.cicid == \"51160\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-------------------+--------+---------+\n",
      "|code|country|average_temperature|latitude|longitude|\n",
      "+----+-------+-------------------+--------+---------+\n",
      "+----+-------+-------------------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countries_df.select(\"*\").where(countries_df.code == \"148\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|code|country|\n",
      "+----+-------+\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i94cit_res_df.select(\"*\").where(i94cit_res_df.code == \"148\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As we can see, there are no matching values for country = 148 in neither the exctracted list nor the dimenssion table we created, which means our data is complete and that our model is working.\n",
    "\n",
    "To avoid such null values, we can update our fact table (i94_df) to subtract all the records with no matching values in the dimenssion tables. Alternatively, we an update our dimenssion tables with the unique values extracted from the fact table. That is, extract the unique lookup values from the fact table and push them to the corresponding dimmension tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can also run some analytical queries to get more insights out of this data. For example, we can get the number of immigrants per season to see if there is more traffic in particular seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|season|  count|\n",
      "+------+-------+\n",
      "|Spring|2493086|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_df = i94_df.join(date_df, i94_df.arrdate == date_df.sasdate)\n",
    "group_df = group_df.groupBy(\"season\").count()\n",
    "group_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|season|  count|\n",
      "+------+-------+\n",
      "|Spring|2223705|\n",
      "|Summer| 147698|\n",
      "|Autumn|  13531|\n",
      "|Winter|      1|\n",
      "+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_df = i94_df.join(date_df, i94_df.depdate == date_df.sasdate)\n",
    "group_df = group_df.groupBy(\"season\").count()\n",
    "group_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We can also get the number of arrivals (or departures) per date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|   isodate|count|\n",
      "+----------+-----+\n",
      "|2016-04-01|87915|\n",
      "|2016-04-02|86499|\n",
      "|2016-04-03|80258|\n",
      "|2016-04-04|78716|\n",
      "|2016-04-05|71279|\n",
      "|2016-04-06|72368|\n",
      "|2016-04-07|81558|\n",
      "|2016-04-08|85140|\n",
      "|2016-04-09|86005|\n",
      "|2016-04-10|83150|\n",
      "|2016-04-11|77843|\n",
      "|2016-04-12|68102|\n",
      "|2016-04-13|72679|\n",
      "|2016-04-14|86135|\n",
      "|2016-04-15|88673|\n",
      "|2016-04-16|95072|\n",
      "|2016-04-17|83690|\n",
      "|2016-04-18|78796|\n",
      "|2016-04-19|68745|\n",
      "|2016-04-20|75635|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "group_df = i94_df.join(date_df, i94_df.arrdate == date_df.sasdate)\n",
    "group_df = group_df.groupBy(\"isodate\").count()\n",
    "group_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "These are just simple test queries to validate the correctness of our data model, but more complex queries can be written to get more insights out of our data, if we have a better environment such as Amazon EMR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
